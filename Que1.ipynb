{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final BLEU Score: 0.0161\n",
    "Final ROUGE-1: 0.0288, ROUGE-2: 0.0267, ROUGE-L: 0.0288\n",
    "Final Perplexity: 45.6756\n",
    "Final Semantic Similarity: 0.1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ Final Evaluation Metrics:\n",
    "Final BLEU Score: 0.0896\n",
    "Final ROUGE-1: 0.1318, ROUGE-2: 0.1032, ROUGE-L: 0.1233\n",
    "Final Perplexity: nan\n",
    "Final Semantic Similarity: 0.3218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:26:54.130870Z",
     "iopub.status.busy": "2025-03-07T15:26:54.130513Z",
     "iopub.status.idle": "2025-03-07T15:27:06.756304Z",
     "shell.execute_reply": "2025-03-07T15:27:06.754630Z",
     "shell.execute_reply.started": "2025-03-07T15:26:54.130844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.2.4)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=865d0d97da140d4e93af476fb606a0212f54eaeacccae118180a70afaf64ee7a\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=00a66b4759731b4723fef8c91478d6a8d9c14bacaece251a7214b707e93d4852\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-07T15:42:40.051828Z",
     "iopub.status.busy": "2025-03-07T15:42:40.051600Z",
     "iopub.status.idle": "2025-03-07T15:42:41.567662Z",
     "shell.execute_reply": "2025-03-07T15:42:41.566803Z",
     "shell.execute_reply.started": "2025-03-07T15:42:40.051807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post: My post is still not aporoved. Ive post 10 times already\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Oh, fuck off...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: nan\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: You don't even need to go that extreme. There would just be a lot more male prostitutes if the demand for Chads was anywhere as close to the demand men have for women.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Literally her second accident resulting in a death in a decade.\n",
      "\n",
      "Was high, doing 180km/hr (110mph), looking at her phone whilst disqualified from driving, in a stolen car and evading arrest.\n",
      "\n",
      "That sentence is so insufficient it is insulting to the victims family.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: The Judas release is the way to go, I think. As uncensored as possible until a BR is available.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Plenty of women like or at least don't mind hairy men.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: I remember having an existential meltdown once I had life served to me on a silver platter. Multiple girlfriends; I lived to fuck and smoke free weed these hoes were buying me. I was being worshipped like a god.\n",
      "\n",
      "I had never been so miserable in my life. Shattered it all so I can go back home and find my true passion. (Music) Having been fucked out and served all the female validation my little former beta self could ever ask for years, Ive got all the time and fuck all to care about girls. I just want to get educated as a backup plan and preform my own concerts.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: I swear no girl looks at me, whenever I try to make eye contact I see them actively looking away.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: my valentine's day present to myself will be getting my eyeliner tattooed :)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Another mindless rant, unhelpful to the uninitiated.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Magnum's Threesome Game and Lay Report [FR]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Woman Who Falsely Accused Teenager Of Rape Will Spend Just One Night In Jail, Mother Speaks Out. (Oh and the child pornography charge was reduced so she won't be considered a sex offender and he probably spent more time incarcerated than she).\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Hahahaha\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: I generally agree but there is evidence for fairly large personality differences between men and women:\n",
      "\n",
      "*\"The Distance Between Mars and Venus: Measuring Global Sex Differences in Personality\"*. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029265\n",
      "\n",
      "In particular, women are far more sensitive and \"easily offended\" than men are, which probably goes a long way towards explaining the stereotype about women being more emotional than men.\n",
      "\n",
      "It's hard to tell if this is biological or social but in the real world these differences are pretty pronounced.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Little Women Atelier has some of the dreamiest feminine linen dresses, Im saving up for my next one and a matching apron! I also frequent Hearts and Found since I enjoy the 1950s style. Both of these shops can be found on Etsy. My tips would be ease into it, I used to be uncomfortable going out in dresses (felt overdressed), let alone skirts that went past my knee. I bought some cheaper transition pieces from department shops until I built my confidence and particular style up! Good luck!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: While i think she should be punished hard for this, I dont think we should compare drugging and robbing to drugging and raping.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Strength Matters, Weak Men Are Not Men | A Stoics Guide to Virtue\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: imagination is so powerful, sometimes you have to be present and embrace reality, it's often much sweeter than you imagine :)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Abuse is abuse, regardless of who's the one doing the abusing\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: You notice it more after reading trp, that doesnt mean that the world is changing\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: It means your SMV is way higher than hers.  It's bad for you because you're settling with someone who is likely pretty unattractive.  Do better.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: I REALLY needed to hear this.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: I think you just have a gene that makes people think suicide is the only way because three deaths ? Wow\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Youre fighting yourself. Some people are more compatible with each other than others. Thats a good thing. Dont pour your heart out to her, but dont hide your feelings. Theres nothing wrong with only talking to one girl as long as you preoccupy yourself and stay busy.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Yup. They just keep getting pickier and pickier. It goes from dont be fat -> be thin -> good hip to waist ratio -> long legs -> big butt/breasts -> cellulite and it goes on. A classmate called a girl's birthmark on her arm \"kinda gross\" one time. And they say women are the shallow ones\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Reddit is rotten to the core.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Reason #143252353 to avoid women.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Not to mention, they'd be at least 10 times more effective than men according to movies and video games. Grrrrrrlll powa!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: nan\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Youre a nobody. Youre not getting #metood, get a grip. Just dont be a sperg and say youre gonna rape her or something retarded.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: My two favorite things about this post are how he unnecessarily details his future niece's ethnic makeup and how he shows his work at the end.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Most of those suicide victims are men.\n",
      "\n",
      ">When domestic violence related suicides are combined with domestic violence homicides, the total numbers of domestic violence related deaths are higher for males than females.\n",
      "\n",
      "######Davis, R. L. (2010). Domestic violence-related deaths. *Journal of aggression, conflict and peace research*, 2(2), 44.\n",
      "https://www.emerald.com/insight/content/doi/10.5042/jacpr.2010.0141/full/html\n",
      "\n",
      "They also left out emotional, financial, and legal abuse.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: This is bullshit\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: She expects me to buy photos from her\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Ok, thats a bit sexist\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Cried in my Zoom Class todayWhy do people have to be so cruel and nasty? I suffer the reverse halo-effect. People always assume that I'm retarded & think I'm weird/ annoying.  I make people physically uncomfortable. \n",
      "\n",
      "My class had a debate on zoom today, and I struggled with public speaking (as usual). One girl had her microphone on & she laughed at me when I stuttered for a moment & forgot my argument. Everyone in my class thinks I'm massively retarded, and they will speak over me or completely ignore me. \n",
      "\n",
      "During the debate this one guy kept picking on me and being rude/ condescending. I noticed a stark difference in the way he spoke to me v.s the prettiest girls in class. It is pure suifuel, & makes me feel like complete shit\n",
      "\n",
      "Our professor asked us how the debate went at the end of class. This one girl from my group (who really dislikes me) openly stated that \"certain people\" in our group are \"incredibly frustrating\" and she \"couldn't stand\" listening to them. Those were her words, almost verbatim. Everyone just laughed...but it was clear she was talking about me. There are only THREE ppl in our group. I don't know why she had to be so nasty and passive aggressive. I am nothing but nice to her. I guess ugly people are garbage though & worthy of no respect \n",
      "\n",
      " I waited until the zoom session ended to speak with my teacher about my struggles with class participation/ public speaking. I immediately choked up and stated to cry before I could say 2 words. My teacher just sat there with this awkward smile on her face and asked if I wanted to \"come back in 2 minutes.\"\n",
      "\n",
      "We then had this INCREDIBLY awkward conversation. It felt like I was holding her hostage lol. Even my own teacher thinks I'm a creepy weirdo. I explained my autism & struggles with social anxiety, and she suddenly started talking about how many students find the course material\" unfamiliar\" and \"intimidating.\" She couldn't understand why I was upset..... You should have seen how uncomfortable she was lol.\n",
      "\n",
      "That was one of the most awkward moments of my life. She looked like she thought I was fake crying. I really wasn't though...I'm just that mentally unstable lol\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Thought she was trans for a bit.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: The combat and extra content isn't the issue.\n",
      "\n",
      "It's the plot being changed.\n",
      "\n",
      "Do some more googling. Nomura is pulling some KH3D shit in this.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: OP, do not listen to this. This guy is taking his experience and assuming its the norm. Its not.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: That's literally the point... haha\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Men DGAF what the enemy thinks of him.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Ha! We definitely had struggles, especially after the second. I can't promise it will pass since I don't know you, but it will if you're both open to communicating your wants and needs and receptive to what the other I saying. It took the same \"discussion\" (read: argument) like 8-10 times before we made any progress. It's still tougher than before kids, but it's also arguably as good as it's ever been for us. \n",
      "\n",
      "Long story short: communication.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Depends on your body and other habits.\n",
      "\n",
      "I either have to bust every day or lift. Otherwise I can't sleep and start to rage.\n",
      "\n",
      "Lifting is better. Instead of trying to fuck everything I just start getting shit done like a maniac when I don't bust and ALSO lift. \n",
      "\n",
      "Lifting is like breathing, life ends without it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: It doesnt count as a try either. I have red (but didnt do) this : Go to a mall. Make eye contact and smile to people while you are walking around.  Next Day you can add a \"good morning\" to them.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Man, if she's a butterface then I'm a goblin-ogre hybrid that got punched in the face repeatedly and got dragged by the hair through the swamps afterwards.  Fuckin christ, rocket high standards much with these dudes.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: thanks. I'm glad you enjoyed my story.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: What was most profound in this book? I havent ever heard of it, but Im compiling a list of books to purchase during quarantine.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Overreporting seems to be the name of the game when it comes  to surveys that question how much work anyone has done in an environment of shared work. The work we've done ourselves feels much more salient than the work someone else has done.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Post: Yeah, she even admits to her early-20s her ride on the carousel! ;-)\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/final-labels/final_labels.csv\") \n",
    "\n",
    "conversation_tree = defaultdict(list)\n",
    "entry_map = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    entry_id = row['entry_id']\n",
    "    parent_id = row['parent_id']\n",
    "    link_id = row['link_id']\n",
    "    text = row['body']\n",
    "    \n",
    "    entry_map[entry_id] = text  # Store text\n",
    "    \n",
    "    # Identify replies\n",
    "    if not pd.isna(parent_id):\n",
    "        conversation_tree[parent_id].append(entry_id)\n",
    "    elif not pd.isna(link_id):\n",
    "        conversation_tree[link_id].append(entry_id)\n",
    "\n",
    "# Identify root posts\n",
    "root_posts = list(set(entry_map.keys()) - set(conversation_tree.keys()))[:100]\n",
    "\n",
    "\n",
    "def print_conversation(entry_id, depth=0):\n",
    "    if entry_id not in entry_map:\n",
    "        return\n",
    "    \n",
    "    prefix = \"Post: \" if depth == 0 else \"Reply: \"\n",
    "    print(\"  \" * depth + f\"{prefix}{entry_map[entry_id]}\")\n",
    "    \n",
    "    for child_id in conversation_tree.get(entry_id, []):\n",
    "        print_conversation(child_id, depth + 1)\n",
    "\n",
    "# Print first 100 conversations\n",
    "for entry_id in root_posts[:50]:\n",
    "    print_conversation(entry_id)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:47:41.342475Z",
     "iopub.status.busy": "2025-03-07T15:47:41.342138Z",
     "iopub.status.idle": "2025-03-07T15:47:47.627264Z",
     "shell.execute_reply": "2025-03-07T15:47:47.626509Z",
     "shell.execute_reply.started": "2025-03-07T15:47:41.342447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 100, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Post 1: Do you have the skin of a 80 year old grandma? Worry no more, just drink water!\n",
      "\n",
      "Summary 1: Do you have the skin of a 80 year old grandma? Worry no more, just drink water!\n",
      "\n",
      "================================================================================\n",
      "Original Post 2: This is taking a grain of truth and extrapolating to insanity.\n",
      "\n",
      "Stay hydrated, it's healthy, you'll look and feel better. It will not reverse the aging process though.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 2: Stay hydrated, it's healthy, you'll look and feel better. It will not reverse the aging process though. This is taking a grain of truth and extrapolating to insanity.\n",
      "\n",
      "================================================================================\n",
      "Original Post 3: Honestly my favorite thing about this is that they feel the need to cite beauty professionals in order to prove that dehydration is caused by not drinking enough water.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 3: \"I'm not sure why they feel the need to cite beauty professionals in order to prove that dehydration is caused by not drinking enough water,\" she said. \"My favorite thing about this is that they feel like they need to cites beauty professionals\"\n",
      "\n",
      "================================================================================\n",
      "Original Post 4: Source? Doesnt sound right to me idk\n",
      "\n",
      "Summary 4: Source? Doesnt sound right to me idk\n",
      "\n",
      "================================================================================\n",
      "Original Post 5: Damn, I saw a movie in which the old woman bathed in the blood if virgins  to do this.  How did no one tell her she just needed some water.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 5: \"I saw a movie in which the old woman bathed in the blood if virgins  to do this,\" he said. \"How did no one tell her she just needed some water?\"\n",
      "\n",
      "================================================================================\n",
      "Original Post 6: It's a question of the sales pitch involved.\n",
      "\n",
      "Obviously, the people from Big Virgin had a better deal.\n",
      "\n",
      "Summary 6: It's a question of the sales pitch involved.\n",
      "\n",
      "Obviously, the people from Big Virgin had a better deal.\n",
      "\n",
      "================================================================================\n",
      "Original Post 7: Some places have poor water quality. Virgin blood may have been less expensive than imported water.\n",
      "\n",
      "Summary 7: Some places have poor water quality. Virgin blood may have been less expensive than imported water.\n",
      "\n",
      "================================================================================\n",
      "Original Post 8: So if I drink enough water I turn into a baby?\n",
      "\n",
      "Summary 8: So if I drink enough water I turn into a baby?\n",
      "\n",
      "================================================================================\n",
      "Original Post 9: You'll Benjamin Button yourself, yes.\n",
      "\n",
      "Summary 9: You'll Benjamin Button yourself, yes.\n",
      "\n",
      "================================================================================\n",
      "Original Post 10: Isn't this the plot of Cocoon?\n",
      "\n",
      "Summary 10: Isn't this the plot of Cocoon?\n",
      "\n",
      "================================================================================\n",
      "Original Post 11: No idea. I watched the movie 10 years or so ago with my dad and all I can remember are the doctors being shocked that the lady was pregnant \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 11: No idea. I watched the movie 10 years or so ago with my dad and all I can remember are the doctors being shocked that the lady was pregnant.\n",
      "\n",
      "================================================================================\n",
      "Original Post 12: In the movie there is this group of elderly that discover that they feel suspiciously rejuvenated by going for a bath in a special swimming pool. I was just being silly with the water/de-aging association.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 12: I was just being silly with the water/de-aging association. In the movie there is this group of elderly that discover that they feel suspiciously rejuvenated by going for a bath in a special swimming pool.\n",
      "\n",
      "================================================================================\n",
      "Original Post 13: One of my friends looks like he's frikkin dead. Been trying for years to pour water down his throat and get him to wash his face. Recently won the face washing battle and he looks 5 years younger.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 13: One of my friends looks like he's frikkin dead. Been trying for years to pour water down his throat and get him to wash his face. Recently won the face washing battle and he looks 5 years younger.\n",
      "\n",
      "================================================================================\n",
      "Original Post 14: What kinda Tom Brady nonsense is this\n",
      "\n",
      "Summary 14: What kinda Tom Brady nonsense is this\n",
      "\n",
      "================================================================================\n",
      "Original Post 15: Recommended by F.F\n",
      "\n",
      "Summary 15: Recommended by F.F\n",
      "\n",
      "================================================================================\n",
      "Original Post 16: Professionals say, that dehydration is caused by not drinking enough water. In other news, water is wet, and oranges are orange.\n",
      "\n",
      "Summary 16: Dehydration is caused by not drinking enough water. Water is wet, and oranges are orange. In other news, water is wet and water is orange.\n",
      "\n",
      "================================================================================\n",
      "Original Post 17: Virgina Spread \n",
      "\n",
      "Summary 17: Virgina Spread \n",
      "\n",
      "================================================================================\n",
      "Original Post 18: *I can't believe it's not* virgina spread open\n",
      "\n",
      "Summary 18: *I can't believe it's not* virgina spread open\n",
      "\n",
      "================================================================================\n",
      "Original Post 19: I hate you because its the middle of the night and Im silently suffering from laughter but your updoot\n",
      "\n",
      "Summary 19: I hate you because its the middle of the night and Im silently suffering from laughter but your updoot\n",
      "\n",
      "================================================================================\n",
      "Original Post 20: Even if you chug a handle of bad vodka and give this person the benefit of the doubt, what?\n",
      "\n",
      "Summary 20: Even if you chug a handle of bad vodka and give this person the benefit of the doubt, what?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def abstractive_summary(text):\n",
    "    \n",
    "    if len(text.split()) < 20:  # Skip very short posts\n",
    "        return text\n",
    "    \n",
    "    summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "file_path = \"/kaggle/input/final-labels/final_labels.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct column exists\n",
    "if 'body' not in df.columns:\n",
    "    raise ValueError(\"Column 'post' not found in dataset. Check CSV structure.\")\n",
    "\n",
    "# Summarize and print each post\n",
    "for idx, post in enumerate(df[\"body\"].dropna()[:20]):\n",
    "    print(f\"Original Post {idx+1}: {post}\\n\")\n",
    "    print(f\"Summary {idx+1}: {abstractive_summary(str(post))}\\n\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:47:04.316590Z",
     "iopub.status.busy": "2025-03-07T15:47:04.316289Z",
     "iopub.status.idle": "2025-03-07T15:47:10.973550Z",
     "shell.execute_reply": "2025-03-07T15:47:10.972640Z",
     "shell.execute_reply.started": "2025-03-07T15:47:04.316569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=494a86012799e253e75de96a955390f5baef4064c7ef557e053da81ffc8f2e1c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T16:10:19.433105Z",
     "iopub.status.busy": "2025-03-07T16:10:19.432639Z",
     "iopub.status.idle": "2025-03-07T16:10:33.184433Z",
     "shell.execute_reply": "2025-03-07T16:10:33.183554Z",
     "shell.execute_reply.started": "2025-03-07T16:10:19.433080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 200, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "Your max_length is set to 200, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Your max_length is set to 200, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 200, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "Your max_length is set to 200, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 200, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 200, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 200, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 200, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "Your max_length is set to 200, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 200, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 200, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Your max_length is set to 200, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 200, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 200, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff94b1efec224feba8d447f98693f505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9963aabbca74678bc57812554aa3968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final BLEU Score: 0.0161\n",
      "Final ROUGE-1: 0.0288, ROUGE-2: 0.0267, ROUGE-L: 0.0288\n",
      "Final Perplexity: 45.6756\n",
      "Final Semantic Similarity: 0.1038\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import math\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def abstractive_summary(text):\n",
    "    \"\"\"Generate an abstractive summary using BART\"\"\"\n",
    "    if len(text.split()) < 20:  # Skip very short posts\n",
    "        return \"nan\"\n",
    "    \n",
    "    summary = summarizer(text, max_length=200, min_length=20, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"Compute corpus-level BLEU Score\"\"\"\n",
    "    references = [[ref.split()] for ref in references]\n",
    "    hypotheses = [hyp.split() for hyp in hypotheses]\n",
    "    return corpus_bleu(references, hypotheses)\n",
    "\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    \"\"\"Compute ROUGE Score for entire dataset\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougeL = [], [], []\n",
    "\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        if ref.strip() and hyp.strip():\n",
    "            scores = scorer.score(ref, hyp)\n",
    "            rouge1.append(scores['rouge1'].fmeasure)\n",
    "            rouge2.append(scores['rouge2'].fmeasure)\n",
    "            rougeL.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    return np.mean(rouge1), np.mean(rouge2), np.mean(rougeL)\n",
    "\n",
    "\n",
    "def calculate_perplexity(sentences):\n",
    "    \"\"\"Compute Perplexity using GPT-2\"\"\"\n",
    "    text = \" \".join(sentences)  # Join all summaries into one text\n",
    "    tokens_tensor = gpt2_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(tokens_tensor, labels=tokens_tensor)\n",
    "        loss = outputs.loss.item()\n",
    "        perplexity = math.exp(loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_semantic_similarity(references, hypotheses):\n",
    "    \"\"\"Compute Semantic Similarity\"\"\"\n",
    "    embeddings1 = semantic_model.encode(references, convert_to_tensor=True)\n",
    "    embeddings2 = semantic_model.encode(hypotheses, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    return torch.mean(similarities).item()\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/kaggle/input/final-labels/final_labels.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure correct columns exist\n",
    "if 'body' not in df.columns or 'highlight' not in df.columns:\n",
    "    raise ValueError(\"Columns 'body' and 'highlight' not found in dataset. Check CSV structure.\")\n",
    "\n",
    "# Process all posts\n",
    "reference_summaries = []\n",
    "generated_summaries = []\n",
    "df=df[:50]\n",
    "for _, row in df.iterrows():\n",
    "    original_post = str(row[\"body\"])\n",
    "    reference_summary = str(row[\"highlight\"])\n",
    "\n",
    "    generated_summary = abstractive_summary(original_post)\n",
    "\n",
    "    if generated_summary != \"nan\":\n",
    "        reference_summaries.append(reference_summary)\n",
    "        generated_summaries.append(generated_summary)\n",
    "\n",
    "# Compute aggregated metrics\n",
    "bleu_score = calculate_bleu(reference_summaries, generated_summaries)\n",
    "rouge1, rouge2, rougeL = calculate_rouge(reference_summaries, generated_summaries)\n",
    "perplexity = calculate_perplexity(generated_summaries)\n",
    "semantic_similarity = calculate_semantic_similarity(reference_summaries, generated_summaries)\n",
    "\n",
    "# Print final results\n",
    "print(f\"Final BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"Final ROUGE-1: {rouge1:.4f}, ROUGE-2: {rouge2:.4f}, ROUGE-L: {rougeL:.4f}\")\n",
    "print(f\"Final Perplexity: {perplexity:.4f}\")\n",
    "print(f\"Final Semantic Similarity: {semantic_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T16:16:38.438083Z",
     "iopub.status.busy": "2025-03-07T16:16:38.437683Z",
     "iopub.status.idle": "2025-03-07T16:35:45.123053Z",
     "shell.execute_reply": "2025-03-07T16:35:45.122274Z",
     "shell.execute_reply.started": "2025-03-07T16:16:38.438055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763423d7eede4db6aea434665902e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/752 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Epoch 1...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 06:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Epoch 2...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 06:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training Epoch 3...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 06:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_bart/tokenizer_config.json',\n",
       " 'fine_tuned_bart/special_tokens_map.json',\n",
       " 'fine_tuned_bart/vocab.json',\n",
       " 'fine_tuned_bart/merges.txt',\n",
       " 'fine_tuned_bart/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "file_path = \"/kaggle/input/final-labels/final_labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "df = df.dropna(subset=[\"body\", \"highlight\"])  # Drop if 'body' or 'highlight' is NaN\n",
    "df = df[df[\"highlight\"].str.strip() != \"\"]  # Remove rows where 'highlight' is just empty spaces\n",
    "\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"body\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlight\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_data = split_dataset[\"train\"]\n",
    "test_data = split_dataset[\"test\"]  \n",
    "\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\", \n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  \n",
    ")\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def train(self, *args, **kwargs):\n",
    "        for epoch in range(int(training_args.num_train_epochs)):\n",
    "            print(f\"\\nðŸš€ Training Epoch {epoch + 1}...\\n\")\n",
    "            super().train(*args, **kwargs)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save Model\n",
    "model.save_pretrained(\"fine_tuned_bart\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_bart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T16:46:20.337136Z",
     "iopub.status.busy": "2025-03-07T16:46:20.336782Z",
     "iopub.status.idle": "2025-03-07T17:04:52.786742Z",
     "shell.execute_reply": "2025-03-07T17:04:52.785481Z",
     "shell.execute_reply.started": "2025-03-07T16:46:20.337115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 0\n",
      "âš ï¸ Skipping empty summary at index 1\n",
      "âš ï¸ Skipping empty summary at index 2\n",
      "âš ï¸ Skipping empty summary at index 3\n",
      "âš ï¸ Skipping empty summary at index 4\n",
      "âš ï¸ Skipping empty summary at index 5\n",
      "âš ï¸ Skipping empty summary at index 6\n",
      "âš ï¸ Skipping empty summary at index 7\n",
      "âš ï¸ Skipping empty summary at index 8\n",
      "âš ï¸ Skipping empty summary at index 9\n",
      "âš ï¸ Skipping empty summary at index 10\n",
      "âš ï¸ Skipping empty summary at index 11\n",
      "âš ï¸ Skipping empty summary at index 12\n",
      "âš ï¸ Skipping empty summary at index 13\n",
      "âš ï¸ Skipping empty summary at index 14\n",
      "âš ï¸ Skipping empty summary at index 15\n",
      "âš ï¸ Skipping empty summary at index 16\n",
      "âš ï¸ Skipping empty summary at index 17\n",
      "âš ï¸ Skipping empty summary at index 18\n",
      "âš ï¸ Skipping empty summary at index 19\n",
      "âš ï¸ Skipping empty summary at index 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12d9f1fa28c411aa3c981a5e060ca9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb1264ce67f48a89a9c74534ae3903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 22\n",
      "âš ï¸ Skipping empty summary at index 23\n",
      "âš ï¸ Skipping empty summary at index 24\n",
      "âš ï¸ Skipping empty summary at index 25\n",
      "âš ï¸ Skipping empty summary at index 26\n",
      "âš ï¸ Skipping empty summary at index 27\n",
      "âš ï¸ Skipping empty summary at index 28\n",
      "âš ï¸ Skipping empty summary at index 29\n",
      "âš ï¸ Skipping empty summary at index 30\n",
      "âš ï¸ Skipping empty summary at index 31\n",
      "âš ï¸ Skipping empty summary at index 32\n",
      "âš ï¸ Skipping empty summary at index 33\n",
      "âš ï¸ Skipping empty summary at index 34\n",
      "âš ï¸ Skipping empty summary at index 35\n",
      "âš ï¸ Skipping empty summary at index 36\n",
      "âš ï¸ Skipping empty summary at index 37\n",
      "âš ï¸ Skipping empty summary at index 38\n",
      "âš ï¸ Skipping empty summary at index 39\n",
      "âš ï¸ Skipping empty summary at index 40\n",
      "âš ï¸ Skipping empty summary at index 41\n",
      "âš ï¸ Skipping empty summary at index 42\n",
      "âš ï¸ Skipping empty summary at index 43\n",
      "âš ï¸ Skipping empty summary at index 44\n",
      "âš ï¸ Skipping empty summary at index 45\n",
      "âš ï¸ Skipping empty summary at index 46\n",
      "âš ï¸ Skipping empty summary at index 47\n",
      "âš ï¸ Skipping empty summary at index 48\n",
      "âš ï¸ Skipping empty summary at index 49\n",
      "âš ï¸ Skipping empty summary at index 50\n",
      "âš ï¸ Skipping empty summary at index 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71f66b4037640849d59676a07cf55bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fb940bd2f7483e9c007ad356f4a49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 53\n",
      "âš ï¸ Skipping empty summary at index 54\n",
      "âš ï¸ Skipping empty summary at index 55\n",
      "âš ï¸ Skipping empty summary at index 56\n",
      "âš ï¸ Skipping empty summary at index 57\n",
      "âš ï¸ Skipping empty summary at index 58\n",
      "âš ï¸ Skipping empty summary at index 59\n",
      "âš ï¸ Skipping empty summary at index 60\n",
      "âš ï¸ Skipping empty summary at index 61\n",
      "âš ï¸ Skipping empty summary at index 62\n",
      "âš ï¸ Skipping empty summary at index 63\n",
      "âš ï¸ Skipping empty summary at index 64\n",
      "âš ï¸ Skipping empty summary at index 65\n",
      "âš ï¸ Skipping empty summary at index 66\n",
      "âš ï¸ Skipping empty summary at index 67\n",
      "âš ï¸ Skipping empty summary at index 68\n",
      "âš ï¸ Skipping empty summary at index 69\n",
      "âš ï¸ Skipping empty summary at index 70\n",
      "âš ï¸ Skipping empty summary at index 71\n",
      "âš ï¸ Skipping empty summary at index 72\n",
      "âš ï¸ Skipping empty summary at index 73\n",
      "âš ï¸ Skipping empty summary at index 74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab23f9717e944a595d250754fd01a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0277227f62e04667b1fa2c35f147d291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 76\n",
      "âš ï¸ Skipping empty summary at index 77\n",
      "âš ï¸ Skipping empty summary at index 78\n",
      "âš ï¸ Skipping empty summary at index 79\n",
      "âš ï¸ Skipping empty summary at index 80\n",
      "âš ï¸ Skipping empty summary at index 81\n",
      "âš ï¸ Skipping empty summary at index 82\n",
      "âš ï¸ Skipping empty summary at index 83\n",
      "âš ï¸ Skipping empty summary at index 84\n",
      "âš ï¸ Skipping empty summary at index 85\n",
      "âš ï¸ Skipping empty summary at index 86\n",
      "âš ï¸ Skipping empty summary at index 87\n",
      "âš ï¸ Skipping empty summary at index 88\n",
      "âš ï¸ Skipping empty summary at index 89\n",
      "âš ï¸ Skipping empty summary at index 90\n",
      "âš ï¸ Skipping empty summary at index 91\n",
      "âš ï¸ Skipping empty summary at index 92\n",
      "âš ï¸ Skipping empty summary at index 93\n",
      "âš ï¸ Skipping empty summary at index 94\n",
      "âš ï¸ Skipping empty summary at index 95\n",
      "âš ï¸ Skipping empty summary at index 96\n",
      "âš ï¸ Skipping empty summary at index 97\n",
      "âš ï¸ Skipping empty summary at index 98\n",
      "âš ï¸ Skipping empty summary at index 99\n",
      "âš ï¸ Skipping empty summary at index 100\n",
      "âš ï¸ Skipping empty summary at index 101\n",
      "âš ï¸ Skipping empty summary at index 102\n",
      "âš ï¸ Skipping empty summary at index 103\n",
      "âš ï¸ Skipping empty summary at index 104\n",
      "âš ï¸ Skipping empty summary at index 105\n",
      "âš ï¸ Skipping empty summary at index 106\n",
      "âš ï¸ Skipping empty summary at index 107\n",
      "âš ï¸ Skipping empty summary at index 108\n",
      "âš ï¸ Skipping empty summary at index 109\n",
      "âš ï¸ Skipping empty summary at index 110\n",
      "âš ï¸ Skipping empty summary at index 111\n",
      "âš ï¸ Skipping empty summary at index 112\n",
      "âš ï¸ Skipping empty summary at index 113\n",
      "âš ï¸ Skipping empty summary at index 114\n",
      "âš ï¸ Skipping empty summary at index 115\n",
      "âš ï¸ Skipping empty summary at index 116\n",
      "âš ï¸ Skipping empty summary at index 117\n",
      "âš ï¸ Skipping empty summary at index 118\n",
      "âš ï¸ Skipping empty summary at index 119\n",
      "âš ï¸ Skipping empty summary at index 120\n",
      "âš ï¸ Skipping empty summary at index 121\n",
      "âš ï¸ Skipping empty summary at index 122\n",
      "âš ï¸ Skipping empty summary at index 123\n",
      "âš ï¸ Skipping empty summary at index 124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba797d0eefa94777ad67412dbc7c3256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324d0e8e067e4b07b6d6a15af1e376f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 126\n",
      "âš ï¸ Skipping empty summary at index 127\n",
      "âš ï¸ Skipping empty summary at index 128\n",
      "âš ï¸ Skipping empty summary at index 129\n",
      "âš ï¸ Skipping empty summary at index 130\n",
      "âš ï¸ Skipping empty summary at index 131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350f8a4236414f7ebdd3bac1dfc42e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1e8072e1746e896187e4521557a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping empty summary at index 133\n",
      "âš ï¸ Skipping empty summary at index 134\n",
      "âš ï¸ Skipping empty summary at index 135\n",
      "âš ï¸ Skipping empty summary at index 136\n",
      "âš ï¸ Skipping empty summary at index 137\n",
      "âš ï¸ Skipping empty summary at index 138\n",
      "âš ï¸ Skipping empty summary at index 139\n",
      "âš ï¸ Skipping empty summary at index 140\n",
      "âš ï¸ Skipping empty summary at index 141\n",
      "âš ï¸ Skipping empty summary at index 142\n",
      "âš ï¸ Skipping empty summary at index 143\n",
      "âš ï¸ Skipping empty summary at index 144\n",
      "âš ï¸ Skipping empty summary at index 145\n",
      "âš ï¸ Skipping empty summary at index 146\n",
      "âš ï¸ Skipping empty summary at index 147\n",
      "âš ï¸ Skipping empty summary at index 148\n",
      "âš ï¸ Skipping empty summary at index 149\n",
      "âš ï¸ Skipping empty summary at index 150\n",
      "\n",
      "ðŸ”¹ Final Evaluation Metrics:\n",
      "Final BLEU Score: 0.0896\n",
      "Final ROUGE-1: 0.1318, ROUGE-2: 0.1032, ROUGE-L: 0.1233\n",
      "Final Perplexity: nan\n",
      "Final Semantic Similarity: 0.3218\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# âœ… Load Fine-Tuned Model from Checkpoint (Step 228)\n",
    "checkpoint_path = \"/kaggle/working/fine_tuned_bart\"\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def generate_summary(text):\n",
    "    \"\"\"Generate summary using fine-tuned BART\"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output = model.generate(input_ids, max_length=128, min_length=30, do_sample=False)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU Score\"\"\"\n",
    "    reference_tokens = [reference.split()]\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens)\n",
    "\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"Compute ROUGE Score\"\"\"\n",
    "    if not reference.strip() or not hypothesis.strip():\n",
    "        return {\"rouge-1\": 0, \"rouge-2\": 0, \"rouge-l\": 0}\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return {\n",
    "        \"rouge-1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge-2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rouge-l\": scores[\"rougeL\"].fmeasure\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_perplexity(sentence):\n",
    "    \"\"\"Compute Perplexity using GPT-2\"\"\"\n",
    "    if not sentence.strip(): \n",
    "        return float(\"inf\")  # Assign a high perplexity for empty outputs\n",
    "\n",
    "    tokens_tensor = gpt2_tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(tokens_tensor, labels=tokens_tensor)\n",
    "        loss = outputs.loss.item()\n",
    "        perplexity = math.exp(loss)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "\n",
    "def calculate_semantic_similarity(reference, hypothesis):\n",
    "    \"\"\"Compute Semantic Similarity\"\"\"\n",
    "    ref_embedding = semantic_model.encode(reference, convert_to_tensor=True)\n",
    "    hyp_embedding = semantic_model.encode(hypothesis, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(ref_embedding, hyp_embedding).item()\n",
    "\n",
    "\n",
    "# Initialize accumulators for final scores\n",
    "total_bleu = 0\n",
    "total_rouge_1 = 0\n",
    "total_rouge_2 = 0\n",
    "total_rouge_l = 0\n",
    "total_perplexity = 0\n",
    "total_similarity = 0\n",
    "num_samples = 0\n",
    "\n",
    "# Evaluate on test set\n",
    "for idx, row in test_data.to_pandas().iterrows():\n",
    "    original_post = str(row[\"body\"])\n",
    "    reference_summary = str(row[\"highlight\"])\n",
    "\n",
    "    generated_summary = generate_summary(original_post)\n",
    "\n",
    "    if not generated_summary.strip():  \n",
    "        print(f\"âš ï¸ Skipping empty summary at index {idx}\")\n",
    "        continue  # âœ… Skip empty summaries\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu_score = calculate_bleu(reference_summary, generated_summary)\n",
    "    rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "    perplexity = calculate_perplexity(generated_summary)\n",
    "    semantic_similarity = calculate_semantic_similarity(reference_summary, generated_summary)\n",
    "\n",
    "    # Accumulate scores\n",
    "    total_bleu += bleu_score\n",
    "    total_rouge_1 += rouge_scores[\"rouge-1\"]\n",
    "    total_rouge_2 += rouge_scores[\"rouge-2\"]\n",
    "    total_rouge_l += rouge_scores[\"rouge-l\"]\n",
    "    total_perplexity += perplexity\n",
    "    total_similarity += semantic_similarity\n",
    "    num_samples += 1\n",
    "\n",
    "\n",
    "# Compute final averages\n",
    "final_bleu = total_bleu / num_samples\n",
    "final_rouge_1 = total_rouge_1 / num_samples\n",
    "final_rouge_2 = total_rouge_2 / num_samples\n",
    "final_rouge_l = total_rouge_l / num_samples\n",
    "final_perplexity = total_perplexity / num_samples\n",
    "final_similarity = total_similarity / num_samples\n",
    "\n",
    "# Print final scores\n",
    "print(\"\\nðŸ”¹ Final Evaluation Metrics:\")\n",
    "print(f\"Final BLEU Score: {final_bleu:.4f}\")\n",
    "print(f\"Final ROUGE-1: {final_rouge_1:.4f}, ROUGE-2: {final_rouge_2:.4f}, ROUGE-L: {final_rouge_l:.4f}\")\n",
    "print(f\"Final Perplexity: {final_perplexity:.4f}\")\n",
    "print(f\"Final Semantic Similarity: {final_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6812832,
     "sourceId": 10952146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
